import akshare as ak
import pandas as pd
from datetime import datetime, timedelta
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
from typing import Callable, Dict, Any, List
import pandas_ta as ta
import numpy as np
import xlsxwriter

# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)


# ==============================================================================
# æ ¸å¿ƒå·¥å…·å‡½æ•°å’Œé…ç½®
# ==============================================================================

def calculate_macd_dynamic_status(df: pd.DataFrame, diff_col: str, prefix: str) -> pd.DataFrame:
    """
    è®¡ç®— MACD DIF çº¿çš„åŠ¨æ€çŠ¶æ€ï¼ˆæ–œç‡å’ŒåŒºåŸŸï¼‰ã€‚

    å‚æ•°:
    - df: åŒ…å« MACD DIF çº¿çš„ DataFrameã€‚
    - diff_col: MACD DIF çº¿çš„åˆ—åï¼ˆä¾‹å¦‚ï¼š'MACD_12_26_DIF'ï¼‰ã€‚
    - prefix: ç”¨äºç»“æœåˆ—å‘½åï¼Œå¦‚ 'MACD_12269'ã€‚

    è¿”å›:
    - åŒ…å« 'åŠ¨æ€çŠ¶æ€' (DYN) å’Œ 'DIF æ–œç‡' (SLOPE) çš„ DataFrame (ä»…è¿”å›æœ€æ–°ä¸€è¡Œ)ã€‚
    """
    # 1. è®¡ç®— DIF çº¿çš„æ–œç‡ (ä¸€é˜¶å·®åˆ†)
    slope_col = f'{prefix}_SLOPE'
    # ä½¿ç”¨ diff(periods=1) è®¡ç®—æ–œç‡
    df[slope_col] = df[diff_col].diff(periods=1)

    # 2. å®šä¹‰åŠ¨æ€çŠ¶æ€åˆ¤æ–­å‡½æ•° (ä½¿ç”¨å‘é‡åŒ–æ“ä½œ np.select)
    status_col = f'{prefix}_DYN'

    # å®šä¹‰æ¡ä»¶
    conditions = [
        # 1. åŠ¨èƒ½åŠ é€Ÿ: DIF > 0 ä¸” Slope >= 0 (å¤„äºå¼ºåŠ¿åŒºåŸŸï¼ŒåŠ¨èƒ½å¢å¼ºæˆ–ä¿æŒ)
        (df[diff_col] > 0) & (df[slope_col] >= 0),
        # 2. åŠ¨èƒ½è¡°å‡ (è­¦ç¤º): DIF > 0 ä¸” Slope < 0 (å¤„äºå¼ºåŠ¿åŒºåŸŸï¼ŒåŠ¨èƒ½å¼€å§‹å‡å¼±)
        (df[diff_col] > 0) & (df[slope_col] < 0),
        # 3. åŠ¨èƒ½ç­‘åº•: DIF < 0 ä¸” Slope >= 0 (å¤„äºå¼±åŠ¿åŒºåŸŸï¼ŒåŠ¨èƒ½å¼€å§‹å¢å¼ºæˆ–ä¿æŒ)
        (df[diff_col] < 0) & (df[slope_col] >= 0),
        # 4. åŠ¨èƒ½æ¶åŒ–: DIF < 0 ä¸” Slope < 0 (å¤„äºå¼±åŠ¿åŒºåŸŸï¼ŒåŠ¨èƒ½åŠ é€Ÿä¸‹é™)
        (df[diff_col] < 0) & (df[slope_col] < 0)
    ]

    # å®šä¹‰ç»“æœ
    choices = [
        'ğŸŸ¢ åŠ¨èƒ½åŠ é€Ÿ',
        'âš ï¸ åŠ¨èƒ½è¡°å‡',
        'ğŸŸ¡ åŠ¨èƒ½ç­‘åº•',
        'ğŸ”´ åŠ¨èƒ½æ¶åŒ–'
    ]

    # åº”ç”¨é€‰æ‹©é€»è¾‘
    df[status_col] = np.select(conditions, choices, default='---')

    # 3. æå–æœ€æ–°çš„ç»“æœ (å³å‰ä¸€ä¸ªäº¤æ˜“æ—¥æ”¶ç›˜æ•°æ®)
    latest_result = df.tail(1)[[status_col, slope_col]]

    # 4. é‡å‘½ååˆ—å¹¶è¿”å›
    latest_result.columns = [status_col, slope_col]
    return latest_result.reset_index(drop=True)

class Config:
    """ç¨‹åºé…ç½®ç±»"""

    def __init__(self):
        self.HOME_DIRECTORY = os.path.expanduser('~')
        self.SAVE_DIRECTORY = os.path.join(self.HOME_DIRECTORY, 'Downloads', 'CoreNews_Reports')
        self.TEMP_DATA_DIRECTORY = os.path.join(self.SAVE_DIRECTORY, 'ShareData')
        self.DATA_FETCH_RETRIES = 3
        self.DATA_FETCH_DELAY = 5
        self.MAX_WORKERS = 15
        self.CODE_ALIASES = {'ä»£ç ': 'è‚¡ç¥¨ä»£ç ', 'è¯åˆ¸ä»£ç ': 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ä»£ç ': 'è‚¡ç¥¨ä»£ç '}
        # åˆ«åå·²åŒ…å«æ‰€æœ‰å¸¸è§åç§°
        self.NAME_ALIASES = {'åç§°': 'è‚¡ç¥¨ç®€ç§°', 'è‚¡ç¥¨åç§°': 'è‚¡ç¥¨ç®€ç§°', 'è‚¡ç¥¨ç®€ç§°': 'è‚¡ç¥¨ç®€ç§°', 'ç®€ç§°': 'è‚¡ç¥¨ç®€ç§°',
                             'ç®€': 'è‚¡ç¥¨ç®€ç§°', 'è¯åˆ¸åç§°': 'è‚¡ç¥¨ç®€ç§°'}
        # === ä»·æ ¼åˆ«åä¿®å¤ v5ï¼šæ¶µç›–æ›´å¤šå®æ—¶è¡Œæƒ…ä»·æ ¼å­—æ®µ ===
        self.PRICE_ALIASES = {'æœ€æ–°ä»·': 'æœ€æ–°ä»·', 'ç°ä»·': 'æœ€æ–°ä»·', 'å½“å‰ä»·æ ¼': 'æœ€æ–°ä»·', 'ä»Šæ”¶ç›˜': 'æœ€æ–°ä»·',
                              'æ”¶ç›˜': 'æœ€æ–°ä»·', 'æ”¶ç›˜ä»·': 'æœ€æ–°ä»·'}


def format_stock_code(code: str) -> str:
    """æ ¹æ®è‚¡ç¥¨ä»£ç çš„å¼€å¤´æ•°å­—ï¼Œæ·»åŠ å¸‚åœºå‰ç¼€ã€‚"""
    code_str = str(code).zfill(6)
    if code_str.startswith('6'):
        return 'sh' + code_str
    elif code_str.startswith(('0', '3')):
        return 'sz' + code_str
    elif code_str.startswith(('4', '8')):
        return 'bj' + code_str
    return code_str

# ==============================================================================
# æ ¸å¿ƒåˆ†æç±» (é›†æˆ Fetching, Processing, Reporting)
# ==============================================================================
class StockAnalyzer:

    def __init__(self):
        self.config = Config()
        self.today_str = datetime.now().strftime("%Y%m%d")
        self.temp_dir = self.config.TEMP_DATA_DIRECTORY
        os.makedirs(self.temp_dir, exist_ok=True)
        self.executor = ThreadPoolExecutor(max_workers=self.config.MAX_WORKERS)
        self.start_time = time.time()

    def _get_file_path(self, base_name: str, cleaned: bool = False) -> str:
        """ç”Ÿæˆä¸´æ—¶æ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚"""
        suffix = "_ç»æ¸…æ´—" if cleaned else ""
        file_name = f"{base_name}{suffix}_{self.today_str}.txt"
        return os.path.join(self.temp_dir, file_name)

    def _load_data_from_cache(self, file_path: str) -> pd.DataFrame:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®ã€‚"""
        if os.path.exists(file_path):
            try:
                # ä½¿ç”¨ '|' åˆ†éš”ç¬¦ï¼Œå¹¶ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯å­—ç¬¦ä¸²æ ¼å¼
                df = pd.read_csv(file_path, sep='|', encoding='utf-8', dtype={'è‚¡ç¥¨ä»£ç ': str})
                print(f"  - å‘ç°ç¼“å­˜ï¼ŒåŠ è½½: {os.path.basename(file_path)}")
                return df
            except Exception as e:
                print(f"[WARN] åŠ è½½ç¼“å­˜ {os.path.basename(file_path)} å¤±è´¥: {e}ï¼Œå°†é‡æ–°è·å–ã€‚")
        return pd.DataFrame()

    def _save_data_to_cache(self, df: pd.DataFrame, file_path: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜ã€‚"""
        try:
            df.to_csv(file_path, sep='|', index=False, encoding='utf-8')
        except Exception as e:
            print(f"[ERROR] ä¿å­˜æ•°æ®åˆ°ç¼“å­˜ {os.path.basename(file_path)} å¤±è´¥: {e}")

    def _safe_ak_fetch(self, fetch_func: Callable, file_base_name: str, **kwargs: Any) -> pd.DataFrame:
        """å¸¦é‡è¯•çš„ AkShare æ•°æ®è·å–ã€‚"""
        cleaned_file_path = self._get_file_path(file_base_name, cleaned=True)
        cached_df = self._load_data_from_cache(cleaned_file_path)
        if not cached_df.empty:
            return cached_df

        df = pd.DataFrame()
        for i in range(self.config.DATA_FETCH_RETRIES):
            try:
                print(f"  - æ­£åœ¨å°è¯•ç¬¬ {i + 1}/{self.config.DATA_FETCH_RETRIES} æ¬¡è·å–æ•°æ®: {file_base_name}...")
                df = fetch_func(**kwargs)
                if df is not None and not df.empty:
                    break
                else:
                    print(f"[WARN] æ•°æ®è¿”å›ä¸ºç©ºæˆ–æ— æ•ˆ: {file_base_name}ï¼Œé‡è¯•ä¸­ã€‚")
                    time.sleep(self.config.DATA_FETCH_DELAY)
            except Exception as e:
                print(f"[ERROR] è·å– {file_base_name} æ—¶å‡ºé”™: {e}ï¼Œå°†åœ¨ {self.config.DATA_FETCH_DELAY} ç§’åé‡è¯•ã€‚")
                time.sleep(self.config.DATA_FETCH_DELAY)

        if df.empty:
            print(f"[FATAL] æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè¿”å›ç©º DataFrame: {file_base_name}")
            return pd.DataFrame()

        cleaned_df = self._clean_and_standardize(df, file_base_name)
        if not cleaned_df.empty:
            self._save_data_to_cache(cleaned_df, cleaned_file_path)

        return cleaned_df

    def _clean_and_standardize(self, df: pd.DataFrame, df_name: str) -> pd.DataFrame:
        """é€šç”¨æ•°æ®æ¸…æ´—å’Œåˆ—åæ ‡å‡†åŒ–ã€‚"""
        if df.empty: return df

        # 1. æ ‡å‡†åŒ–åˆ—åï¼šä½¿ç”¨ Config ä¸­çš„åˆ«åæ˜ å°„
        for old, new in self.config.CODE_ALIASES.items():
            if old in df.columns: df.rename(columns={old: new}, inplace=True)
        for old, new in self.config.NAME_ALIASES.items():
            if old in df.columns: df.rename(columns={old: new}, inplace=True)
        for old, new in self.config.PRICE_ALIASES.items():
            if old in df.columns: df.rename(columns={old: new}, inplace=True)

        if 'è‚¡ç¥¨ä»£ç ' not in df.columns:
            return pd.DataFrame()

        # 2. æ¸…æ´—æ•°æ®
        df.dropna(subset=['è‚¡ç¥¨ä»£ç '], inplace=True)
        df.drop_duplicates(subset=['è‚¡ç¥¨ä»£ç '], inplace=True)
        df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)

        if 'æœ€æ–°ä»·' in df.columns:
            # ä½¿ç”¨ errors='coerce' å°†ä»»ä½•éæ•°å­—å€¼å¼ºåˆ¶è½¬æ¢ä¸º NaNï¼Œç¡®ä¿åˆ—ç±»å‹ä¸º float
            df['æœ€æ–°ä»·'] = pd.to_numeric(df['æœ€æ–°ä»·'], errors='coerce')

        # 3. è¿‡æ»¤STè‚¡ (ä¾èµ– 'è‚¡ç¥¨ç®€ç§°' åˆ—)
        if 'è‚¡ç¥¨ç®€ç§°' not in df.columns:
            cleaned_df = df.copy()
        else:
            cleaned_df = df[~df['è‚¡ç¥¨ç®€ç§°'].str.contains('ST|st|é€€å¸‚|bj|BJ', case=False, na=False)].copy()

        if len(cleaned_df) == 0:
            print(f"[WARN] {df_name} æ•°æ®æ¸…æ´—åä¸ºç©ºã€‚")
            return pd.DataFrame()

        return cleaned_df

    def _get_all_raw_data(self) -> Dict[str, pd.DataFrame]:
        """é›†ä¸­è·å–æ‰€æœ‰æ•°æ®æºã€‚ (æ­¤å¤„çœç•¥éƒ¨åˆ† AkShare è°ƒç”¨å®ç°)"""
        print("\n>>> æ­£åœ¨åˆå§‹åŒ–æ•°æ®è·å–å’Œç¼“å­˜æ£€æŸ¥...")
        # (æ­¤å¤„ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼Œä»¥ä¸“æ³¨äºæ ¸å¿ƒé›†æˆç‚¹)
        data = {
            'spot_data_all': self._safe_ak_fetch(ak.stock_zh_a_spot, "Aè‚¡å®æ—¶è¡Œæƒ…"),
            'main_report_raw': self._safe_ak_fetch(ak.stock_profit_forecast_em, "ä¸»åŠ›ç ”æŠ¥ç›ˆåˆ©é¢„æµ‹"),
            # ... å…¶ä»–æ•°æ®è·å– (å·²åœ¨åŸä»£ç ä¸­å®šä¹‰) ...
            'market_fund_flow_raw': self._safe_ak_fetch(ak.stock_fund_flow_individual, "5æ—¥å¸‚åœºèµ„é‡‘æµå‘", symbol="5æ—¥æ’è¡Œ"),
            'market_fund_flow_raw_10': self._safe_ak_fetch(ak.stock_fund_flow_individual, "10æ—¥å¸‚åœºèµ„é‡‘æµå‘", symbol="10æ—¥æ’è¡Œ"),
            'market_fund_flow_raw_20': self._safe_ak_fetch(ak.stock_fund_flow_individual, "20æ—¥å¸‚åœºèµ„é‡‘æµå‘", symbol="20æ—¥æ’è¡Œ"),
        }
        # å‡è®¾è¿™é‡Œè°ƒç”¨äº†è·å–è¡Œä¸šæ¿å—å’Œå‡çº¿çªç ´çš„é€»è¾‘ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ data ä¸­
        data['industry_board_df'] = pd.DataFrame()
        data['top_industry_cons_df'] = pd.DataFrame()
        data['processed_xstp_df'] = pd.DataFrame() # å‡çº¿å¤šå¤´æ’åˆ—æ•°æ®

        return data

    def _fetch_hist_data_parallel(self, codes: List[str], days: int) -> pd.DataFrame:
        """å¹¶è¡Œè·å–æŒ‡å®šè‚¡ç¥¨çš„å†å²æ•°æ® (æ­¤å¤„ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼Œä»¥ä¸“æ³¨äºæ ¸å¿ƒé›†æˆç‚¹)"""
        print(f"\næ­£åœ¨ä¸º {len(codes)} åªè‚¡ç¥¨ä¸‹è½½ {days} å¤©çš„å†å²æ•°æ®...")

        # ä»…ä¸ºæ¼”ç¤º TA è®¡ç®—ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«éšæœºæ•°æ®çš„ç®€åŒ– DF
        # å®é™…ä»£ç ä¸­éœ€è¦å®ç° ak.stock_zh_a_hist 
        if not codes:
            return pd.DataFrame()
        
        # æ¨¡æ‹Ÿå†å²æ•°æ®ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„è¡Œå’Œæ­£ç¡®çš„åˆ—å
        all_data = []
        for code in codes:
            df_len = 50 
            df = pd.DataFrame({
                'date': pd.to_datetime(pd.date_range(end=datetime.now(), periods=df_len, closed='left')),
                'open': np.random.rand(df_len) * 100,
                'high': np.random.rand(df_len) * 100 + 1,
                'low': np.random.rand(df_len) * 100 - 1,
                'close': np.random.rand(df_len) * 100,
            })
            df['è‚¡ç¥¨ä»£ç '] = code
            df['close'] = df['close'].sort_index(ascending=True).cumsum() + 10 # ç¡®ä¿è¶‹åŠ¿
            df['open'] = df['close'].shift(1).fillna(10)
            df['high'] = df[['close', 'open']].max(axis=1) + 0.1
            df['low'] = df[['close', 'open']].min(axis=1) - 0.1
            all_data.append(df)
            
        return pd.concat(all_data, ignore_index=True)


    def _custom_macd(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®— MACD (12, 26, 9) å’Œ MACD (6, 13, 5)ï¼Œå¹¶æ ‡è®°é‡‘å‰ä¿¡å·ã€‚
        """
        close = df['close']

        def calculate_single_macd(df, fast, slow, signal):
            """å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—å¹¶è¿”å›å¸¦æœ‰ä¿¡å·çš„DataFrame"""

            # 1. ä½¿ç”¨ pandas_ta è®¡ç®— MACD ç›¸å…³çš„åˆ—
            # é»˜è®¤åˆ—åï¼šMACD_12_26_9, MACDh_12_26_9, MACDs_12_26_9
            # æˆ‘ä»¬éœ€è¦ DIF çº¿ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸ºï¼šMACD (DIF) - MACDs (DEA)
            # ç®€åŒ–è®¡ç®—ï¼Œç›´æ¥ä½¿ç”¨ EMA
            
            # ä½¿ç”¨ EWM è®¡ç®— EMA
            ema_fast = close.ewm(span=fast, adjust=False).mean()
            ema_slow = close.ewm(span=slow, adjust=False).mean()

            # 2. è®¡ç®— DIF
            dif_col = f'MACD_{fast}_{slow}_DIF'
            df[dif_col] = ema_fast - ema_slow

            # 3. è®¡ç®— DEA
            dea_col = f'MACD_{fast}_{slow}_DEA'
            df[dea_col] = df[dif_col].ewm(span=signal, adjust=False).mean()

            # 4. åˆ¤æ–­ MACD é‡‘å‰ä¿¡å· (DIF ä¸Šç©¿ DEA) å’Œé›¶è½´ä½ç½®
            signal_col = f'MACD_{fast}{slow}{signal}_SIGNAL_DETAIL'
            is_cross = (df[dif_col] > df[dea_col]) & (df[dif_col].shift(1).fillna(0) <= df[dea_col].shift(1).fillna(0))

            # åŒºåˆ†é›¶è½´ä¸Š/ä¸‹é‡‘å‰
            df[signal_col] = np.where(
                is_cross,
                np.where(
                    # é›¶è½´ä¸Šé‡‘å‰æ¡ä»¶: é‡‘å‰å‘ç”Ÿï¼Œä¸” DIF å’Œ DEA å‡å¤§äº 0
                    (df[dif_col] > 0) & (df[dea_col] > 0),
                    'é›¶è½´ä¸Šé‡‘å‰', # å¼ºåŠ¿é‡‘å‰
                    'é›¶è½´ä¸‹é‡‘å‰' # å¼±åŠ¿é‡‘å‰
                ),
                '' # éé‡‘å‰ä¿¡å·ä¸ºç©º
            )

            df.rename(columns={signal_col: f'MACD_{fast}{slow}{signal}_SIGNAL_DETAIL'}, inplace=True)
            return df

        # æ ‡å‡† MACD (12, 26, 9)
        df = calculate_single_macd(df, 12, 26, 9)
        # åŠ é€Ÿ MACD (6, 13, 5)
        df = calculate_single_macd(df, 6, 13, 5)

        return df

    def _classify_cci_level(self, cci_value: float) -> str:
        """å°† CCI å€¼åˆ†ç±»ä¸ºä¸“ä¸šçŠ¶æ€ (æ­¤å¤„ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼Œä»¥ä¸“æ³¨äºæ ¸å¿ƒé›†æˆç‚¹)"""
        if cci_value > 200:
            return f"æåº¦è¶…ä¹° ({cci_value:.2f})"
        elif cci_value > 100:
            return f"è¶…ä¹° ({cci_value:.2f})"
        elif cci_value < -200:
            return f"æåº¦è¶…å– ({cci_value:.2f})"
        elif cci_value < -100:
            return f"è¶…å– ({cci_value:.2f})"
        return ''

    def _fetch_and_calculate_ta_signals(self, code: str, hist_df_all: pd.DataFrame, ta_signals: Dict[str, List[Dict]]) -> None:
        """ä¸ºå•ä¸ªè‚¡ç¥¨è®¡ç®—å¹¶æå–æŠ€æœ¯æŒ‡æ ‡ä¿¡å·ã€‚"""
        # æå–å½“å‰è‚¡ç¥¨çš„å†å²æ•°æ®å‰¯æœ¬
        df = hist_df_all[hist_df_all['è‚¡ç¥¨ä»£ç '] == code].copy()
        if df.empty or len(df) < 30:
            return

        # ç¡®ä¿æ‰€éœ€åˆ—æ˜¯æ•°å­—ç±»å‹ (ä½¿ç”¨è‹±æ–‡åˆ—å)
        for col in ['close', 'open', 'high', 'low']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=['close'], inplace=True)
        if df.empty or 'close' not in df.columns or 'open' not in df.columns or 'high' not in df.columns or 'low' not in df.columns:
            return

        try:
            # 1. MACD (é‡‘å‰) - **è®¡ç®—å¹¶æå–ä¸¤ç§å‘¨æœŸçš„ä¿¡å·**
            df = self._custom_macd(df)

            # --- æ–°å¢åŠ¨èƒ½çŠ¶æ€è®¡ç®—ä¸æå– ---
            # è®¡ç®— MACD_12269 åŠ¨èƒ½çŠ¶æ€
            try:
                # ä½¿ç”¨å‰¯æœ¬é˜²æ­¢ä¿®æ”¹åŸå§‹ df
                dyn_12269 = calculate_macd_dynamic_status(
                    df.copy(),
                    diff_col='MACD_12_26_DIF',
                    prefix='MACD_12269'
                )
                # åˆå¹¶è‚¡ç¥¨ä»£ç å’ŒåŠ¨æ€çŠ¶æ€ç»“æœ
                ta_signals['MACD_12269_DYN'].append(
                    {'è‚¡ç¥¨ä»£ç ': code, **dyn_12269.iloc[0].to_dict()}
                )
            except KeyError:
                pass
            except Exception:
                pass

            # è®¡ç®— MACD_6135 åŠ¨èƒ½çŠ¶æ€
            try:
                dyn_6135 = calculate_macd_dynamic_status(
                    df.copy(),
                    diff_col='MACD_6_13_DIF',
                    prefix='MACD_6135'
                )
                ta_signals['MACD_6135_DYN'].append(
                    {'è‚¡ç¥¨ä»£ç ': code, **dyn_6135.iloc[0].to_dict()}
                )
            except KeyError:
                pass
            except Exception:
                pass
            # ----------------------------------------------------

            # æå– MACD 12269 ä¿¡å·
            detail_col_12269 = 'MACD_122699_SIGNAL_DETAIL' # æ³¨æ„ï¼šå–å†³äº _custom_macd çš„å®é™…å®ç°
            if detail_col_12269 in df.columns:
                signal_detail = df[detail_col_12269].iloc[-1]
                if signal_detail != '':
                    ta_signals['MACD_12269'].append({'è‚¡ç¥¨ä»£ç ': code, 'MACD_12269_Signal': signal_detail})

            # æå– MACD 6135 ä¿¡å·
            detail_col_6135 = 'MACD_61355_SIGNAL_DETAIL'
            if detail_col_6135 in df.columns:
                signal_detail = df[detail_col_6135].iloc[-1]
                if signal_detail != '':
                    ta_signals['MACD_6135'].append({'è‚¡ç¥¨ä»£ç ': code, 'MACD_6135_Signal': signal_detail})

            # 2. KDJ (è¶…å–é‡‘å‰) - ä½¿ç”¨ pandas_ta
            df.ta.stoch(append=True, k=9, d=3, smooth_k=3)
            k_col, d_col, j_col = 'STOCHk_9_3_3', 'STOCHd_9_3_3', 'STOCHj_9_3_3'
            
            # (KDJ ä¿¡å·æå–é€»è¾‘çœç•¥ï¼Œå‡è®¾åŸä»£ç å·²å®ç°)
            # ...

            # 3. CCI (ä¸“ä¸šåˆ†ç±») - ä½¿ç”¨ pandas_ta
            df.ta.cci(append=True, close='close', high='high', low='low')
            cci_cols = [col for col in df.columns if col.startswith('CCI_')]
            if cci_cols:
                cci_col = cci_cols[0]
                current_cci = df[cci_col].iloc[-1]
                cci_signal = self._classify_cci_level(current_cci)
                if cci_signal:
                    ta_signals['CCI'].append(
                        {'è‚¡ç¥¨ä»£ç ': code, 'CCI_Signal': cci_signal})

            # 4. RSI (è¶…å–ä½ä½) - ä½¿ç”¨ pandas_ta
            df.ta.rsi(append=True, close='close')
            rsi_cols = [col for col in df.columns if col.startswith('RSI_')]
            if rsi_cols:
                rsi_col = rsi_cols[0]
                if df[rsi_col].iloc[-1] < 30:
                    ta_signals['RSI'].append(
                        {'è‚¡ç¥¨ä»£ç ': code, 'RSI_Signal': f"è¶…å–ä½ä½ ({df[rsi_col].iloc[-1]:.2f})"})

            # 5. BOLL (ä½æ³¢/ç¼©å£) - ä½¿ç”¨ pandas_ta
            df.ta.bbands(append=True, close='close', length=20)
            # (BOLL ä¿¡å·æå–é€»è¾‘çœç•¥ï¼Œå‡è®¾åŸä»£ç å·²å®ç°)
            # ...

        except Exception as e:
            print(f"[ERROR] è®¡ç®— {code} çš„æŠ€æœ¯æŒ‡æ ‡ä¿¡å·æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def _get_ta_signals_parallel(self, codes: List[str], hist_df_all: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """å¹¶è¡Œè·å–æ‰€æœ‰è‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡ä¿¡å·ã€‚"""
        print("\n>>> æ­£åœ¨å¹¶è¡Œè®¡ç®—å’Œæå–æŠ€æœ¯æŒ‡æ ‡ä¿¡å·...")

        # ç¡®ä¿åˆå§‹åŒ–æ‰€æœ‰å¯èƒ½è¿”å›çš„ä¿¡å·å­—å…¸
        ta_signals: Dict[str, List[Dict]] = {
            'MACD_12269': [],
            'MACD_6135': [],
            'KDJ': [],
            'CCI': [],
            'RSI': [],
            'BOLL': [],
            'MACD_12269_DYN': [], # <-- NEW
            'MACD_6135_DYN': [], # <-- NEW
        }

        future_to_code = {
            self.executor.submit(
                self._fetch_and_calculate_ta_signals,
                code,
                hist_df_all,
                ta_signals
            ): code
            for code in codes
        }

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(future_to_code):
            # å¼‚å¸¸å¤„ç†å·²åœ¨ _fetch_and_calculate_ta_signals å†…éƒ¨å¤„ç†
            pass

        # å°†ç»“æœè½¬æ¢ä¸º DataFrame
        processed_ta_dfs: Dict[str, pd.DataFrame] = {}
        for key, value_list in ta_signals.items():
            if value_list:
                processed_ta_dfs[key] = pd.DataFrame(value_list)

        return processed_ta_dfs
    
    # ==========================================================================
    # [ä¿®æ”¹] _consolidate_data: ç¡®ä¿ MACD åŠ¨èƒ½çŠ¶æ€åˆ—è¢«åˆå¹¶
    # ==========================================================================
    def _consolidate_data(self, processed_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """åˆå¹¶æ‰€æœ‰æ•°æ®æºå’Œä¿¡å·ï¼Œç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Šã€‚"""
        print("\n>>> æ­£åœ¨æ±‡æ€»æ‰€æœ‰æ•°æ®å’Œä¿¡å· (æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‹¬ç«‹åˆ—)...")

        # 1. æå–æ‰€æœ‰æœ‰ä¿¡å·çš„è‚¡ç¥¨ä»£ç ä½œä¸ºåŸºç¡€
        all_codes = set()
        data_sources = [
            processed_data.get('processed_main_report'),
            processed_data.get('processed_xstp_df'),
            processed_data.get('market_fund_flow_raw'),
            processed_data.get('strong_stocks_raw'),
            processed_data.get('consecutive_rise_raw'),
            processed_data.get('ljqs_raw'),
            processed_data.get('cxfl_raw'),
            processed_data.get('MACD_12269', pd.DataFrame()),
            processed_data.get('MACD_12269_DYN', pd.DataFrame()), # <-- NEW
            processed_data.get('MACD_6135', pd.DataFrame()),
            processed_data.get('MACD_6135_DYN', pd.DataFrame()), # <-- NEW
            processed_data.get('KDJ', pd.DataFrame()),
            processed_data.get('CCI', pd.DataFrame()),
            processed_data.get('RSI', pd.DataFrame()),
            processed_data.get('BOLL', pd.DataFrame()),
        ]
        for df in data_sources:
            if df is not None and not df.empty and 'è‚¡ç¥¨ä»£ç ' in df.columns:
                all_codes.update(df['è‚¡ç¥¨ä»£ç '].unique())
        
        if not all_codes:
            return pd.DataFrame()
        
        final_df = pd.DataFrame(list(all_codes), columns=['è‚¡ç¥¨ä»£ç '])
        final_df['è‚¡ç¥¨ä»£ç '] = final_df['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)
        
        # 2. åˆå¹¶è¡Œæƒ…ã€ç ”æŠ¥ã€å¸‚åœºã€å‡çº¿æ•°æ® (æ­¤å¤„çœç•¥åˆå¹¶ç»†èŠ‚ï¼Œä»¥ä¸“æ³¨äº TA ä¿¡å·)
        # ... 

        # 3. åˆå¹¶æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡ä¿¡å·
        ta_dfs_to_merge = []

        # 1. MACD 12269
        macd_12269_df = processed_data.get('MACD_12269', pd.DataFrame())
        if not macd_12269_df.empty:
            ta_dfs_to_merge.append(macd_12269_df[['è‚¡ç¥¨ä»£ç ', 'MACD_12269_Signal']].rename(
                columns={'MACD_12269_Signal': 'MACD_12269'}))

        # **NEW** MACD 12269 åŠ¨èƒ½çŠ¶æ€
        macd_12269_dyn_df = processed_data.get('MACD_12269_DYN', pd.DataFrame())
        if not macd_12269_dyn_df.empty:
            ta_dfs_to_merge.append(macd_12269_dyn_df[['è‚¡ç¥¨ä»£ç ', 'MACD_12269_DYN', 'MACD_12269_SLOPE']])


        # 2. MACD 6135
        macd_6135_df = processed_data.get('MACD_6135', pd.DataFrame())
        if not macd_6135_df.empty:
            ta_dfs_to_merge.append(macd_6135_df[['è‚¡ç¥¨ä»£ç ', 'MACD_6135_Signal']].rename(
                columns={'MACD_6135_Signal': 'MACD_6135'}))

        # **NEW** MACD 6135 åŠ¨èƒ½çŠ¶æ€
        macd_6135_dyn_df = processed_data.get('MACD_6135_DYN', pd.DataFrame())
        if not macd_6135_dyn_df.empty:
            ta_dfs_to_merge.append(macd_6135_dyn_df[['è‚¡ç¥¨ä»£ç ', 'MACD_6135_DYN', 'MACD_6135_SLOPE']])


        # 3. KDJ, CCI, RSI, BOLL çš„åˆå¹¶é€»è¾‘ (å‡è®¾å·²å®ç°)
        kdj_df = processed_data.get('KDJ', pd.DataFrame())
        if not kdj_df.empty:
             ta_dfs_to_merge.append(kdj_df[['è‚¡ç¥¨ä»£ç ', 'KDJ_Signal']])

        cci_df = processed_data.get('CCI', pd.DataFrame())
        if not cci_df.empty:
            ta_dfs_to_merge.append(cci_df[['è‚¡ç¥¨ä»£ç ', 'CCI_Signal']])

        rsi_df = processed_data.get('RSI', pd.DataFrame())
        if not rsi_df.empty:
            ta_dfs_to_merge.append(rsi_df[['è‚¡ç¥¨ä»£ç ', 'RSI_Signal']])

        boll_df = processed_data.get('BOLL', pd.DataFrame())
        if not boll_df.empty:
            ta_dfs_to_merge.append(boll_df[['è‚¡ç¥¨ä»£ç ', 'BOLL_Signal']])


        # å°†æ‰€æœ‰ TA ä¿¡å·åˆå¹¶åˆ° final_df
        for ta_df in ta_dfs_to_merge:
            final_df = pd.merge(final_df, ta_df.drop_duplicates(subset=['è‚¡ç¥¨ä»£ç ']), on='è‚¡ç¥¨ä»£ç ', how='left')

        # å¯¹æ–°çš„ TA ä¿¡å·åˆ—å¡«å……ç©ºå€¼
        for col in [
            'MACD_12269', 'MACD_6135',
            'MACD_12269_DYN', 'MACD_12269_SLOPE', # <-- NEW COLUMNS
            'MACD_6135_DYN', 'MACD_6135_SLOPE', # <-- NEW COLUMNS
            'KDJ_Signal', 'CCI_Signal', 'RSI_Signal', 'BOLL_Signal'
        ]:
            if col in final_df.columns:
                # åŠ¨èƒ½çŠ¶æ€å¡«å……ä¸º '---'ï¼Œæ–œç‡å¡«å……ä¸º NaN (ä»¥ä¾¿åç»­ç­›é€‰æˆ–è®¡ç®—è¯„åˆ†)
                if 'SLOPE' in col:
                    # æ–œç‡æ˜¯æ•°å€¼ï¼Œä¸éœ€è¦å¡«å……ä¸º ''ï¼Œä¿æŒ NaN
                    pass
                elif 'DYN' in col:
                    final_df[col] = final_df[col].fillna('---')
                else:
                    final_df[col] = final_df[col].fillna('')
            else:
                if 'SLOPE' in col:
                    final_df[col] = np.nan # æ•°å€¼å‹ç©ºåˆ—
                elif 'DYN' in col:
                    final_df[col] = '---' # å­—ç¬¦å‹ç©ºåˆ—
                else:
                    final_df[col] = ''

        # ... (å…¶ä½™é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾‹å¦‚è¿‡æ»¤å’Œæ’åº)

        return final_df.sort_values(by='è‚¡ç¥¨ä»£ç ').reset_index(drop=True)


    def _generate_excel_report(self, final_df: pd.DataFrame, data: Dict[str, pd.DataFrame]):
        """ç”Ÿæˆ Excel æŠ¥å‘Š (æ­¤å¤„ä½¿ç”¨ç®€åŒ–é€»è¾‘ï¼Œä»¥ä¸“æ³¨äºæ ¸å¿ƒé›†æˆç‚¹)"""
        print("\n>>> æ­£åœ¨ç”Ÿæˆ Excel æŠ¥å‘Š...")
        # å®é™…ä»£ç ä¸­éœ€è¦å®ç°å®Œæ•´çš„ Excel æŠ¥å‘Šç”Ÿæˆé€»è¾‘
        
        report_path = os.path.join(self.config.SAVE_DIRECTORY, f"è‚¡ç¥¨ç­›é€‰æŠ¥å‘Š_{self.today_str}_é›†æˆç‰ˆ.xlsx")
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.config.SAVE_DIRECTORY, exist_ok=True)
        
        # å‡è®¾ï¼šä»…ä¿å­˜æ±‡æ€»è¡¨
        final_df.to_excel(report_path, sheet_name='æ•°æ®æ±‡æ€»', index=False)
        print(f" - æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜åˆ°: {report_path}")

    def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•ã€‚"""
        print(f"è‚¡ç¥¨åˆ†æç¨‹åºå¯åŠ¨ (Today: {self.today_str})")
        
        # æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶ (åªæ¸…ç†å½“æ—¥æ–‡ä»¶ä»¥å¤–çš„)
        for filename in os.listdir(self.temp_dir):
             if not filename.endswith(f"{self.today_str}.txt"):
                 try:
                     os.remove(os.path.join(self.temp_dir, filename))
                 except Exception:
                     pass
        print("æ—§çš„ä¸´æ—¶æ•°æ®æ–‡ä»¶æ¸…ç†å®Œæˆã€‚")

        try:
            # 1. è·å–æ‰€æœ‰åŸå§‹æ•°æ®
            raw_data = self._get_all_raw_data()
            
            # 2. è·å–æ‰€æœ‰æœ‰ä»£ç çš„è‚¡ç¥¨åˆ—è¡¨ (ä½¿ç”¨å®æ—¶è¡Œæƒ…ä½œä¸ºåŸºç¡€)
            spot_df = raw_data.get('spot_data_all', pd.DataFrame())
            if spot_df.empty:
                print("[FATAL] å®æ—¶è¡Œæƒ…æ•°æ®ä¸ºç©ºï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                return
            all_codes = spot_df['è‚¡ç¥¨ä»£ç '].unique().tolist()
            
            # 3. è·å–å†å²æ•°æ®
            # ä»…ä¸‹è½½ MACD, KDJ, CCI, RSI, BOLL æ‰€éœ€çš„å¤©æ•° (ä¾‹å¦‚ 60 å¤©)
            hist_df_all = self._fetch_hist_data_parallel(all_codes, days=60)
            if hist_df_all.empty:
                 print("[WARN] å†å²æ•°æ®è·å–å¤±è´¥æˆ–ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ã€‚")

            # 4. å¹¶è¡Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ä¿¡å·
            ta_signals = self._get_ta_signals_parallel(all_codes, hist_df_all)
            
            # 5. åˆå¹¶å¤„ç†æ•°æ®
            processed_data = {**raw_data, **ta_signals} # å°† TA ä¿¡å·åˆå¹¶åˆ° processed_data
            
            # 6. ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š
            final_df = self._consolidate_data(processed_data)
            
            if not final_df.empty:
                # 7. ç”Ÿæˆ Excel æ–‡ä»¶
                self._generate_excel_report(final_df, raw_data)
                
            print(f"\nç¨‹åºæ€»è€—æ—¶: {time.time() - self.start_time:.2f} ç§’")

        except Exception as e:
            print(f"[FATAL] ç¨‹åºè¿è¡Œå‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")

# ==============================================================================
# è¿è¡Œä¸»ç¨‹åº
# ==============================================================================
if __name__ == '__main__':
    # ä»…åœ¨å®é™…è¿è¡Œæ—¶æ‰§è¡Œï¼Œè¿™é‡Œå¯ä»¥ä½œä¸ºä»£ç å±•ç¤ºçš„ç»“æŸ
    pass
